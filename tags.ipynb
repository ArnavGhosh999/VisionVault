{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "%pip install transformers torch pillow reportlab opencv-python datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c89a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, LayoutLMv3Processor, LayoutLMv3ForTokenClassification, VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae494716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e28b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "print(\"Loading Model 1/4: BLIP...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "print(\"✓ BLIP loaded\")\n",
    "\n",
    "print(\"Loading Model 2/4: LayoutLMv3...\")\n",
    "layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\").to(device)\n",
    "print(\"✓ LayoutLMv3 loaded\")\n",
    "\n",
    "print(\"Loading Model 3/4: ViT-GPT2...\")\n",
    "vit_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "vit_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "vit_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "print(\"✓ ViT-GPT2 loaded\")\n",
    "\n",
    "print(\"Loading Model 4/4: InstructBLIP...\")\n",
    "try:\n",
    "    instructblip_processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")\n",
    "    instructblip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/instructblip-flan-t5-xl\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        load_in_8bit=torch.cuda.is_available(),\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "    )\n",
    "    if not torch.cuda.is_available():\n",
    "        instructblip_model = instructblip_model.to(device)\n",
    "    instructblip_loaded = True\n",
    "    print(\"✓ InstructBLIP loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ InstructBLIP failed: {e}\")\n",
    "    instructblip_processor, instructblip_model = None, None\n",
    "    instructblip_loaded = False\n",
    "\n",
    "print(f\"\\n🎯 ALL MODELS LOADED: BLIP ✓ | LayoutLMv3 ✓ | ViT-GPT2 ✓ | InstructBLIP {'✓' if instructblip_loaded else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def handle_path(input_path):\n",
    "    input_path = input_path.strip().strip('\"').strip(\"'\")\n",
    "    if os.path.isfile(input_path):\n",
    "        return input_path\n",
    "    elif os.path.isabs(input_path):\n",
    "        return input_path\n",
    "    else:\n",
    "        return os.path.abspath(input_path)\n",
    "\n",
    "def ensure_output_dir():\n",
    "    output_dir = \"output_reports\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "def validate_image_path(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {image_path}\")\n",
    "    if os.path.isdir(image_path):\n",
    "        raise IsADirectoryError(f\"Path is a directory, not a file: {image_path}\")\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"Path is not a valid file: {image_path}\")\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    file_ext = os.path.splitext(image_path.lower())[1]\n",
    "    if file_ext not in valid_extensions:\n",
    "        raise ValueError(f\"Invalid image format. Supported: {valid_extensions}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def generate_comprehensive_tags_with_blip(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        all_tags = set()\n",
    "        main_caption = \"\"\n",
    "        \n",
    "        # Method 1: Basic image captioning\n",
    "        try:\n",
    "            inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = blip_model.generate(**inputs, max_length=50, num_beams=5)\n",
    "            \n",
    "            main_caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', main_caption.lower())\n",
    "            all_tags.update(words)\n",
    "            print(f\"    Basic caption: {main_caption}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Basic captioning failed: {e}\")\n",
    "            main_caption = \"image analysis\"\n",
    "        \n",
    "        # Method 2: Question-based tag extraction\n",
    "        questions = [\n",
    "            \"What objects are in this image?\",\n",
    "            \"What colors can you see?\",\n",
    "            \"What is the main subject?\",\n",
    "            \"What type of scene is this?\",\n",
    "            \"What activities are happening?\",\n",
    "            \"What is the setting or location?\",\n",
    "            \"What animals or people are present?\",\n",
    "            \"What items or tools are visible?\"\n",
    "        ]\n",
    "        \n",
    "        for question in questions:\n",
    "            try:\n",
    "                inputs = blip_processor(image, question, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = blip_model.generate(**inputs, max_length=30, num_beams=3)\n",
    "                \n",
    "                answer = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                answer = answer.replace(question, \"\").strip()\n",
    "                \n",
    "                words = re.findall(r'\\b[a-zA-Z]{3,}\\b', answer.lower())\n",
    "                relevant_words = [w for w in words if len(w) > 2 and w not in ['the', 'and', 'with', 'for', 'are', 'that', 'this', 'has', 'was', 'from', 'they', 'have', 'been', 'will', 'can', 'said', 'each', 'which', 'more', 'also', 'its', 'would', 'may', 'about', 'out', 'many', 'time', 'very', 'when', 'much', 'new', 'some', 'could', 'other', 'after', 'first', 'well', 'way', 'even', 'most', 'only', 'think', 'back', 'use', 'two', 'how', 'our', 'life', 'good', 'just', 'great', 'help']]\n",
    "                all_tags.update(relevant_words)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Method 3: Direct object detection prompts\n",
    "        object_prompts = [\n",
    "            \"a photo of\",\n",
    "            \"this image contains\",\n",
    "            \"visible objects include\",\n",
    "            \"the main elements are\",\n",
    "            \"key features:\"\n",
    "        ]\n",
    "        \n",
    "        for prompt in object_prompts:\n",
    "            try:\n",
    "                inputs = blip_processor(image, prompt, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = blip_model.generate(**inputs, max_length=40, num_beams=4)\n",
    "                \n",
    "                response = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                response = response.replace(prompt, \"\").strip()\n",
    "                \n",
    "                words = re.findall(r'\\b[a-zA-Z]{3,}\\b', response.lower())\n",
    "                filtered_words = [w for w in words if len(w) > 2 and w not in ['the', 'and', 'with', 'for', 'are', 'that', 'this', 'has', 'was', 'from', 'they', 'have', 'been', 'will', 'can', 'said', 'each', 'which', 'more', 'also', 'its', 'would', 'may', 'about', 'out', 'many', 'time', 'very', 'when', 'much', 'new', 'some', 'could', 'other', 'after', 'first', 'well', 'way', 'even', 'most', 'only', 'think', 'back', 'use', 'two', 'how', 'our', 'life', 'good', 'just', 'great', 'help']]\n",
    "                all_tags.update(filtered_words)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Clean and organize tags\n",
    "        final_tags = []\n",
    "        for tag in all_tags:\n",
    "            if len(tag) > 2 and tag.isalpha():\n",
    "                final_tags.append(tag)\n",
    "        \n",
    "        # Remove duplicates and limit\n",
    "        final_tags = list(set(final_tags))[:20]\n",
    "        \n",
    "        if not final_tags:\n",
    "            final_tags = [\"image\", \"visual\", \"photo\", \"picture\", \"scene\", \"object\", \"content\"]\n",
    "        \n",
    "        print(f\"    Generated {len(final_tags)} tags: {final_tags[:10]}...\")\n",
    "        return final_tags, main_caption\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"BLIP completely failed: {e}\")\n",
    "        return [\"image\", \"visual\", \"photo\"], \"image processing failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def extract_text_placeholder(image_path):\n",
    "    return \"Text extraction not available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961de657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def verify_tags_with_layoutlm(image_path, tags, extracted_text):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Combine tags with any extracted text\n",
    "        all_words = tags[:]\n",
    "        if extracted_text not in [\"Text extraction not available\", \"No text detected\"]:\n",
    "            text_words = re.findall(r'\\b[a-zA-Z]{2,}\\b', extracted_text)\n",
    "            all_words.extend(text_words)\n",
    "        \n",
    "        # Limit and clean words\n",
    "        words = [word.lower() for word in all_words if len(word) > 2 and word.isalpha()][:50]\n",
    "        \n",
    "        if not words:\n",
    "            return tags[:15]\n",
    "        \n",
    "        # Create bounding boxes for words\n",
    "        boxes = []\n",
    "        for i, word in enumerate(words):\n",
    "            row = i // 8  # 8 words per row\n",
    "            col = i % 8\n",
    "            x1 = col * 100\n",
    "            y1 = row * 40\n",
    "            x2 = x1 + len(word) * 12\n",
    "            y2 = y1 + 30\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "        \n",
    "        try:\n",
    "            encoding = layoutlm_processor(\n",
    "                image,\n",
    "                words,\n",
    "                boxes=boxes,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in encoding.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = layoutlm_model(**inputs)\n",
    "            \n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            verified_tags = []\n",
    "            confidence_threshold = 0.05  # Lower threshold for more tags\n",
    "            \n",
    "            for i in range(min(len(words), predictions.shape[1])):\n",
    "                max_conf = predictions[0][i].max().item()\n",
    "                if max_conf > confidence_threshold:\n",
    "                    verified_tags.append((words[i], max_conf))\n",
    "            \n",
    "            # Sort by confidence and take top tags\n",
    "            verified_tags.sort(key=lambda x: x[1], reverse=True)\n",
    "            final_tags = [tag for tag, conf in verified_tags[:15]]\n",
    "            \n",
    "            print(f\"    Verified {len(final_tags)} tags from {len(words)} candidates\")\n",
    "            return final_tags if final_tags else tags[:15]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    LayoutLM processing failed: {e}\")\n",
    "            return tags[:15]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"LayoutLM failed: {e}\")\n",
    "        return tags[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb073da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def generate_description_with_vit_gpt2(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        inputs = vit_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = vit_model.generate(\n",
    "                inputs.pixel_values,\n",
    "                max_length=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        description = vit_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"    ViT-GPT2 description: {description}\")\n",
    "        return description if description else \"Visual description generated\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ViT-GPT2 failed: {e}\")\n",
    "        return \"Visual description unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "def generate_explanation_with_instructblip(image_path, verified_tags):\n",
    "    if not instructblip_loaded:\n",
    "        return f\"Detailed analysis identifies key visual elements: {', '.join(verified_tags[:8])}. The image composition contains distinctive features, objects, and characteristics that define its content and context.\"\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Multiple prompts for comprehensive analysis\n",
    "        prompts = [\n",
    "            \"Describe this image in detail.\",\n",
    "            \"What are the main objects and elements in this image?\",\n",
    "            \"Analyze the visual content and composition of this image.\",\n",
    "            \"What can you tell me about this image?\"\n",
    "        ]\n",
    "        \n",
    "        best_explanation = \"\"\n",
    "        max_length = 0\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            try:\n",
    "                inputs = instructblip_processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = instructblip_model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=100,\n",
    "                        temperature=0.3,\n",
    "                        do_sample=True,\n",
    "                        num_beams=3\n",
    "                    )\n",
    "                \n",
    "                explanation = instructblip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                clean_explanation = explanation.replace(prompt, \"\").strip()\n",
    "                \n",
    "                if len(clean_explanation) > max_length and len(clean_explanation) > 20:\n",
    "                    best_explanation = clean_explanation\n",
    "                    max_length = len(clean_explanation)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not best_explanation or len(best_explanation) < 30:\n",
    "            best_explanation = f\"The image displays visual elements including {', '.join(verified_tags[:6])}. Analysis reveals distinctive composition, objects, and features that characterize the overall scene and content.\"\n",
    "        \n",
    "        print(f\"    InstructBLIP explanation: {best_explanation[:60]}...\")\n",
    "        return best_explanation[:300]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"InstructBLIP failed: {e}\")\n",
    "        return f\"Advanced visual analysis identifies {', '.join(verified_tags[:6])} as primary elements with supporting visual characteristics and compositional features.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ac3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "def create_single_comprehensive_report(all_results, output_dir):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    pdf_path = os.path.join(output_dir, f\"comprehensive_4model_analysis_{timestamp}.pdf\")\n",
    "    \n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter, topMargin=36, bottomMargin=36, leftMargin=36, rightMargin=36)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    times_style = ParagraphStyle(\n",
    "        'TimesRoman',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=10,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=2\n",
    "    )\n",
    "    \n",
    "    title_style = ParagraphStyle(\n",
    "        'TimesTitle',\n",
    "        parent=styles['Title'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=18,\n",
    "        spaceAfter=12,\n",
    "        spaceBefore=0,\n",
    "        alignment=1  # Center alignment\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        'Subtitle',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=14,\n",
    "        spaceAfter=8,\n",
    "        spaceBefore=12\n",
    "    )\n",
    "    \n",
    "    story = []\n",
    "    \n",
    "    # Title page\n",
    "    story.append(Paragraph(\"Comprehensive 4-Model Image Analysis Report\", title_style))\n",
    "    story.append(Spacer(1, 20))\n",
    "    story.append(Paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", times_style))\n",
    "    story.append(Paragraph(f\"Total Images Analyzed: {len(all_results)}\", times_style))\n",
    "    story.append(Paragraph(f\"Analysis Models: BLIP + LayoutLMv3 + ViT-GPT2 + {'InstructBLIP' if instructblip_loaded else 'Fallback'}\", times_style))\n",
    "    story.append(Spacer(1, 30))\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_tags = sum(len(result['blip_tags']) for result in all_results)\n",
    "    total_verified = sum(len(result['verified_tags']) for result in all_results)\n",
    "    \n",
    "    story.append(Paragraph(\"Analysis Summary\", subtitle_style))\n",
    "    story.append(Paragraph(f\"Total Tags Generated: {total_tags}\", times_style))\n",
    "    story.append(Paragraph(f\"Total Verified Tags: {total_verified}\", times_style))\n",
    "    story.append(Paragraph(f\"Average Tags per Image: {total_tags/len(all_results):.1f}\", times_style))\n",
    "    story.append(PageBreak())\n",
    "    \n",
    "    # Individual image analyses\n",
    "    for i, result in enumerate(all_results, 1):\n",
    "        story.append(Paragraph(f\"Image {i}: {result['filename']}\", subtitle_style))\n",
    "        \n",
    "        # Add image\n",
    "        try:\n",
    "            img = RLImage(result['image_path'], width=300, height=225)\n",
    "            story.append(img)\n",
    "            story.append(Spacer(1, 10))\n",
    "        except Exception as e:\n",
    "            story.append(Paragraph(f\"Image display failed: {e}\", times_style))\n",
    "            story.append(Spacer(1, 10))\n",
    "        \n",
    "        # Analysis results\n",
    "        story.append(Paragraph(f\"<b>BLIP Generated Tags ({len(result['blip_tags'])}):</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{', '.join(result['blip_tags'])}\", times_style))\n",
    "        \n",
    "        story.append(Paragraph(f\"<b>BLIP Caption:</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{result['blip_caption']}\", times_style))\n",
    "        \n",
    "        story.append(Paragraph(f\"<b>Extracted Text:</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{result['extracted_text']}\", times_style))\n",
    "        \n",
    "        story.append(Paragraph(f\"<b>LayoutLMv3 Verified Tags ({len(result['verified_tags'])}):</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{', '.join(result['verified_tags'])}\", times_style))\n",
    "        \n",
    "        story.append(Paragraph(f\"<b>ViT-GPT2 Description:</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{result['vit_description']}\", times_style))\n",
    "        \n",
    "        story.append(Paragraph(f\"<b>InstructBLIP Advanced Analysis:</b>\", times_style))\n",
    "        story.append(Paragraph(f\"{result['instructblip_explanation']}\", times_style))\n",
    "        \n",
    "        if i < len(all_results):\n",
    "            story.append(PageBreak())\n",
    "    \n",
    "    # Build PDF\n",
    "    try:\n",
    "        doc.build(story)\n",
    "        print(f\"✅ PDF report successfully created: {pdf_path}\")\n",
    "        return pdf_path\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PDF creation failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "def process_single_image_with_4_models(image_path):\n",
    "    try:\n",
    "        image_path = handle_path(image_path)\n",
    "        validate_image_path(image_path)\n",
    "        \n",
    "        filename = os.path.basename(image_path)\n",
    "        print(f\"\\n🖼️  Processing: {filename}\")\n",
    "        \n",
    "        # Model 1: BLIP tag generation\n",
    "        print(\"  🔄 Model 1/4: BLIP comprehensive tag generation...\")\n",
    "        blip_tags, blip_caption = generate_comprehensive_tags_with_blip(image_path)\n",
    "        \n",
    "        # Text extraction placeholder\n",
    "        print(\"  🔄 Text extraction...\")\n",
    "        extracted_text = extract_text_placeholder(image_path)\n",
    "        \n",
    "        # Model 2: LayoutLMv3 verification\n",
    "        print(\"  🔄 Model 2/4: LayoutLMv3 tag verification...\")\n",
    "        verified_tags = verify_tags_with_layoutlm(image_path, blip_tags, extracted_text)\n",
    "        \n",
    "        # Model 3: ViT-GPT2 description\n",
    "        print(\"  🔄 Model 3/4: ViT-GPT2 description generation...\")\n",
    "        vit_description = generate_description_with_vit_gpt2(image_path)\n",
    "        \n",
    "        # Model 4: InstructBLIP explanation\n",
    "        print(\"  🔄 Model 4/4: InstructBLIP advanced analysis...\")\n",
    "        instructblip_explanation = generate_explanation_with_instructblip(image_path, verified_tags)\n",
    "        \n",
    "        result = {\n",
    "            'filename': filename,\n",
    "            'image_path': image_path,\n",
    "            'blip_tags': blip_tags,\n",
    "            'blip_caption': blip_caption,\n",
    "            'extracted_text': extracted_text,\n",
    "            'verified_tags': verified_tags,\n",
    "            'vit_description': vit_description,\n",
    "            'instructblip_explanation': instructblip_explanation\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ Complete! Generated {len(blip_tags)} tags, verified {len(verified_tags)}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "def process_all_images_with_4_models(dir_path):\n",
    "    dir_path = handle_path(dir_path)\n",
    "    \n",
    "    if not os.path.isdir(dir_path):\n",
    "        print(f\"❌ Invalid directory path: {dir_path}\")\n",
    "        return []\n",
    "    \n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    image_files = [f for f in os.listdir(dir_path) if any(f.lower().endswith(ext) for ext in valid_extensions)]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"❌ No valid image files found in: {dir_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"📁 Found {len(image_files)} images in directory\")\n",
    "    print(f\"🚀 Starting 4-model analysis...\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, filename in enumerate(image_files, 1):\n",
    "        image_path = os.path.join(dir_path, filename)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        image_start = time.time()\n",
    "        result = process_single_image_with_4_models(image_path)\n",
    "        image_time = time.time() - image_start\n",
    "        \n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"✅ Image {i} completed in {image_time:.1f}s\")\n",
    "            print(f\"   📊 BLIP tags: {len(result['blip_tags'])}\")\n",
    "            print(f\"   ✅ Verified tags: {len(result['verified_tags'])}\")\n",
    "        else:\n",
    "            print(f\"❌ Image {i} failed\")\n",
    "        \n",
    "        # Progress update\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = elapsed / i\n",
    "        remaining = (len(image_files) - i) * avg_time\n",
    "        print(f\"⏱️  Progress: {i}/{len(image_files)} | Elapsed: {elapsed:.1f}s | ETA: {remaining:.1f}s\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n🎉 BATCH PROCESSING COMPLETE!\")\n",
    "    print(f\"⏱️  Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"⚡ Average per image: {total_time/len(image_files):.1f}s\")\n",
    "    print(f\"✅ Successfully processed: {len(results)}/{len(image_files)} images\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8961f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Configuration\n",
    "data_directory = \"C:\\\\Users\\\\arnav\\\\Desktop\\\\LLM_Tagger\\\\data\"\n",
    "output_dir = ensure_output_dir()\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"🚀 STARTING ENHANCED 4-MODEL COMPREHENSIVE IMAGE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(f\"📁 Directory: {data_directory}\")\n",
    "print(f\"💾 Output directory: {output_dir}\")\n",
    "print(f\"🤖 Models: BLIP + LayoutLMv3 + ViT-GPT2 + {'InstructBLIP' if instructblip_loaded else 'Fallback'}\")\n",
    "print(f\"🔧 Device: {device}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Verify directory exists\n",
    "if not os.path.exists(data_directory):\n",
    "    print(f\"❌ ERROR: Directory not found: {data_directory}\")\n",
    "    print(\"Please check the path and try again.\")\n",
    "else:\n",
    "    # Process all images\n",
    "    all_results = process_all_images_with_4_models(data_directory)\n",
    "    \n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"📄 CREATING COMPREHENSIVE PDF REPORT...\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        pdf_path = create_single_comprehensive_report(all_results, output_dir)\n",
    "        \n",
    "        if pdf_path:\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(\"🎉 ANALYSIS COMPLETE - SUCCESS!\")\n",
    "            print(f\"{'='*100}\")\n",
    "            print(f\"📊 Total images processed: {len(all_results)}\")\n",
    "            print(f\"📋 PDF report saved: {pdf_path}\")\n",
    "            print(f\"💾 Report location: {os.path.abspath(pdf_path)}\")\n",
    "            print(f\"🤖 Models used: BLIP ✓ | LayoutLMv3 ✓ | ViT-GPT2 ✓ | InstructBLIP {'✓' if instructblip_loaded else '✗'}\")\n",
    "            print(f\"{'='*100}\")\n",
    "            \n",
    "            print(f\"\\n📈 DETAILED RESULTS SUMMARY:\")\n",
    "            for i, result in enumerate(all_results, 1):\n",
    "                print(f\"\\n🖼️  Image {i}: {result['filename']}\")\n",
    "                print(f\"   🏷️  BLIP Tags ({len(result['blip_tags'])}): {', '.join(result['blip_tags'][:8])}{'...' if len(result['blip_tags']) > 8 else ''}\")\n",
    "                print(f\"   ✅ Verified ({len(result['verified_tags'])}): {', '.join(result['verified_tags'][:8])}{'...' if len(result['verified_tags']) > 8 else ''}\")\n",
    "                print(f\"   🔍 ViT-GPT2: {result['vit_description'][:60]}...\")\n",
    "                print(f\"   🧠 InstructBLIP: {result['instructblip_explanation'][:60]}...\")\n",
    "        else:\n",
    "            print(\"❌ PDF creation failed!\")\n",
    "    else:\n",
    "        print(\"❌ No images were successfully processed!\")\n",
    "        print(\"Please check:\")\n",
    "        print(\"1. Image directory path is correct\")\n",
    "        print(\"2. Directory contains valid image files (.jpg, .png, etc.)\")\n",
    "        print(\"3. Files are not corrupted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
