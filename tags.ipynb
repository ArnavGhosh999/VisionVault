{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e74303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: reportlab in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.4.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: datasets in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from reportlab) (3.4.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "%pip install transformers torch pillow reportlab opencv-python datasets accelerate bitsandbytes pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eaa180cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe23a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "259e45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n",
      "Loading LayoutLMv3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "print(\"Loading BLIP model...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "print(\"Loading LayoutLMv3 model...\")\n",
    "layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\").to(device)\n",
    "\n",
    "print(\"All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "542582e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def handle_path(input_path):\n",
    "    input_path = input_path.strip().strip('\"').strip(\"'\")\n",
    "    if os.path.isfile(input_path):\n",
    "        return input_path\n",
    "    elif os.path.isabs(input_path):\n",
    "        return input_path\n",
    "    else:\n",
    "        return os.path.abspath(input_path)\n",
    "\n",
    "def ensure_output_dir():\n",
    "    output_dir = \"output_reports\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    return output_dir\n",
    "\n",
    "def validate_image_path(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {image_path}\")\n",
    "    if os.path.isdir(image_path):\n",
    "        raise IsADirectoryError(f\"Path is a directory, not a file: {image_path}\")\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"Path is not a valid file: {image_path}\")\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    file_ext = os.path.splitext(image_path.lower())[1]\n",
    "    if file_ext not in valid_extensions:\n",
    "        raise ValueError(f\"Invalid image format. Supported: {valid_extensions}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e1fac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def generate_comprehensive_tags(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    prompts = [\n",
    "        \"a photo of\",\n",
    "        \"this image shows\",\n",
    "        \"this is\",\n",
    "        \"the image contains\",\n",
    "        \"visible in this image:\"\n",
    "    ]\n",
    "    \n",
    "    all_tags = set()\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = blip_model.generate(**inputs, max_length=40, num_beams=3, temperature=0.7, do_sample=True)\n",
    "        \n",
    "        caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        caption_clean = caption.replace(prompt, \"\").strip()\n",
    "        \n",
    "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', caption_clean.lower())\n",
    "        exclude_words = {'the', 'and', 'with', 'for', 'are', 'that', 'this', 'has', 'was', 'from', 'they', 'have', 'been', 'will', 'can', 'said', 'each', 'which', 'more', 'also', 'its', 'would', 'may', 'about', 'out', 'many', 'time', 'very', 'when', 'much', 'new', 'some', 'could', 'state', 'other', 'after', 'first', 'well', 'way', 'even', 'years', 'work', 'through', 'over', 'into', 'than', 'because', 'most', 'only', 'think', 'back', 'use', 'two', 'how', 'our', 'life', 'good', 'woman', 'just', 'form', 'great', 'help', 'line', 'turn', 'cause', 'mean', 'before', 'move', 'right', 'old', 'same', 'tell', 'does', 'set', 'three', 'want', 'air', 'play', 'small', 'end', 'put', 'home', 'read', 'hand', 'large', 'add', 'land', 'here', 'must', 'big', 'high', 'such', 'follow', 'act', 'why', 'ask', 'men', 'change', 'went', 'light', 'kind', 'off', 'need', 'house', 'picture', 'try', 'again', 'animal', 'point', 'mother', 'world', 'near', 'build', 'self', 'earth', 'father', 'head', 'stand', 'own', 'page', 'should', 'country', 'found', 'answer', 'school', 'grow', 'study', 'still', 'learn', 'plant', 'cover', 'food', 'sun', 'four', 'between', 'keep', 'eye', 'never', 'last', 'let', 'thought', 'city', 'cross', 'farm', 'hard', 'start', 'might', 'story', 'saw', 'far', 'sea', 'draw', 'left', 'late', 'run', 'while', 'press', 'close', 'night', 'real', 'few', 'north', 'open', 'seem', 'together', 'next', 'white', 'children', 'begin', 'got', 'walk', 'example', 'ease', 'paper', 'group', 'always', 'music', 'those', 'both', 'mark', 'often', 'letter', 'until', 'mile', 'river', 'car', 'feet', 'care', 'second', 'book', 'carry', 'took', 'science', 'eat', 'room', 'friend', 'began', 'idea', 'fish', 'mountain', 'stop', 'once', 'base', 'hear', 'horse', 'cut', 'sure', 'watch', 'color', 'face', 'wood', 'main', 'enough', 'plain', 'girl', 'usual', 'young', 'ready', 'above', 'ever', 'red', 'list', 'though', 'feel', 'talk', 'bird', 'soon', 'body', 'dog', 'family', 'direct', 'leave', 'song', 'measure', 'door', 'product', 'black', 'short', 'numeral', 'class', 'wind', 'question', 'happen', 'complete', 'ship', 'area', 'half', 'rock', 'order', 'fire', 'south', 'problem', 'piece', 'told', 'knew', 'pass', 'since', 'top', 'whole', 'king', 'space', 'heard', 'best', 'hour', 'better', 'during', 'hundred', 'five', 'remember', 'step', 'early', 'hold', 'west', 'ground', 'interest', 'reach', 'fast', 'verb', 'sing', 'listen', 'six', 'table', 'travel', 'less', 'morning', 'ten', 'simple', 'several', 'vowel', 'toward', 'war', 'lay', 'against', 'pattern', 'slow', 'center', 'love', 'person', 'money', 'serve', 'appear', 'road', 'map', 'rain', 'rule', 'govern', 'pull', 'cold', 'notice', 'voice', 'unit', 'power', 'town', 'fine', 'certain', 'fly', 'fall', 'lead', 'cry', 'dark', 'machine', 'note', 'wait', 'plan', 'figure', 'star', 'box', 'noun', 'field', 'rest', 'correct', 'able', 'pound', 'done', 'beauty', 'drive', 'stood', 'contain', 'front', 'teach', 'week', 'final', 'gave', 'green', 'quick', 'develop', 'ocean', 'warm', 'free', 'minute', 'strong', 'special', 'mind', 'behind', 'clear', 'tail', 'produce', 'fact', 'street', 'inch', 'multiply', 'nothing', 'course', 'stay', 'wheel', 'full', 'force', 'blue', 'object', 'decide', 'surface', 'deep', 'moon', 'island', 'foot', 'system', 'busy', 'test', 'record', 'boat', 'common', 'gold', 'possible', 'plane', 'stead', 'dry', 'wonder', 'laugh', 'thousands', 'ago', 'ran', 'check', 'game', 'shape', 'equate', 'hot', 'miss', 'brought', 'heat', 'snow', 'tire', 'bring', 'yes', 'distant', 'fill', 'east', 'paint', 'language', 'among'}\n",
    "        \n",
    "        filtered_words = [word for word in words if word not in exclude_words and len(word) > 2]\n",
    "        all_tags.update(filtered_words)\n",
    "    \n",
    "    final_tags = list(all_tags)[:12]\n",
    "    \n",
    "    basic_inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        basic_outputs = blip_model.generate(**basic_inputs, max_length=30, num_beams=2)\n",
    "    basic_caption = blip_processor.decode(basic_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return final_tags, basic_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6849497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def extract_text_with_ocr(image_path):\n",
    "    try:\n",
    "        import pytesseract\n",
    "        from PIL import Image\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image, lang='eng')\n",
    "        cleaned_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        cleaned_text = ' '.join(cleaned_text.split())\n",
    "        return cleaned_text.strip() if cleaned_text.strip() else \"No text detected\"\n",
    "    except Exception as e:\n",
    "        return \"Text extraction failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "844d33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def verify_tags_with_layoutlm(image_path, tags, extracted_text):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        all_words = tags + extracted_text.split()\n",
    "        words = [word.lower() for word in all_words if word.isalpha() and len(word) > 2][:80]\n",
    "        \n",
    "        if not words:\n",
    "            return tags\n",
    "        \n",
    "        inputs = layoutlm_processor(image, words, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = layoutlm_model(**inputs)\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        verified_tags = []\n",
    "        \n",
    "        for i, word in enumerate(words[:min(len(words), predictions.shape[1])]):\n",
    "            if predictions[0][i].max().item() > 0.2:\n",
    "                verified_tags.append(word)\n",
    "        \n",
    "        unique_verified = list(set(verified_tags))\n",
    "        return unique_verified[:10] if unique_verified else tags[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return tags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1e8d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def generate_simple_explanation(image_path, verified_tags, caption):\n",
    "    try:\n",
    "        explanations = [\n",
    "            f\"This image shows {caption}.\",\n",
    "            f\"The main elements visible include {', '.join(verified_tags[:5])}.\",\n",
    "            f\"Key features identified are {', '.join(verified_tags[:4])} which define the visual content.\",\n",
    "            f\"The image contains {', '.join(verified_tags[:6])} as primary visual elements.\",\n",
    "            f\"Visual analysis reveals {', '.join(verified_tags[:5])} among other distinguishable features.\"\n",
    "        ]\n",
    "        \n",
    "        import random\n",
    "        selected_explanation = random.choice(explanations)\n",
    "        return selected_explanation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"This image contains visual elements including {', '.join(verified_tags[:4])}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d6cc6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "def create_single_comprehensive_report(all_results, output_dir):\n",
    "    pdf_path = os.path.join(output_dir, \"comprehensive_image_analysis_report.pdf\")\n",
    "    \n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter, topMargin=36, bottomMargin=36)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    times_style = ParagraphStyle(\n",
    "        'TimesRoman',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=10,\n",
    "        spaceAfter=4,\n",
    "        spaceBefore=0\n",
    "    )\n",
    "    \n",
    "    title_style = ParagraphStyle(\n",
    "        'TimesTitle',\n",
    "        parent=styles['Title'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=16,\n",
    "        spaceAfter=12,\n",
    "        spaceBefore=0\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        'Subtitle',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=12,\n",
    "        spaceAfter=6,\n",
    "        spaceBefore=8\n",
    "    )\n",
    "    \n",
    "    story = []\n",
    "    story.append(Paragraph(\"Comprehensive Image Analysis Report\", title_style))\n",
    "    story.append(Paragraph(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", times_style))\n",
    "    story.append(Paragraph(f\"Total Images Analyzed: {len(all_results)}\", times_style))\n",
    "    story.append(Spacer(1, 20))\n",
    "    \n",
    "    for i, result in enumerate(all_results, 1):\n",
    "        story.append(Paragraph(f\"Image {i}: {result['filename']}\", subtitle_style))\n",
    "        \n",
    "        try:\n",
    "            img = RLImage(result['image_path'], width=240, height=180)\n",
    "            story.append(img)\n",
    "        except:\n",
    "            story.append(Paragraph(\"Image display failed\", times_style))\n",
    "        \n",
    "        story.append(Spacer(1, 8))\n",
    "        story.append(Paragraph(f\"<b>AI-Generated Tags:</b> {', '.join(result['tags'])}\", times_style))\n",
    "        story.append(Paragraph(f\"<b>BLIP Caption:</b> {result['caption']}\", times_style))\n",
    "        story.append(Paragraph(f\"<b>Extracted Text:</b> {result['extracted_text']}\", times_style))\n",
    "        story.append(Paragraph(f\"<b>Verified Tags:</b> {', '.join(result['verified_tags'])}\", times_style))\n",
    "        story.append(Paragraph(f\"<b>Description:</b> {result['explanation']}\", times_style))\n",
    "        \n",
    "        if i < len(all_results):\n",
    "            story.append(PageBreak())\n",
    "    \n",
    "    doc.build(story)\n",
    "    return pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be9dfccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "def process_single_image(image_path):\n",
    "    try:\n",
    "        image_path = handle_path(image_path)\n",
    "        validate_image_path(image_path)\n",
    "        \n",
    "        print(f\"Processing: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        tags, caption = generate_comprehensive_tags(image_path)\n",
    "        extracted_text = extract_text_with_ocr(image_path)\n",
    "        verified_tags = verify_tags_with_layoutlm(image_path, tags, extracted_text)\n",
    "        explanation = generate_simple_explanation(image_path, verified_tags, caption)\n",
    "        \n",
    "        return {\n",
    "            'filename': os.path.basename(image_path),\n",
    "            'image_path': image_path,\n",
    "            'tags': tags,\n",
    "            'caption': caption,\n",
    "            'extracted_text': extracted_text,\n",
    "            'verified_tags': verified_tags,\n",
    "            'explanation': explanation\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5dc6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "def process_all_images_in_directory(dir_path):\n",
    "    dir_path = handle_path(dir_path)\n",
    "    \n",
    "    if not os.path.isdir(dir_path):\n",
    "        print(\"Invalid directory path\")\n",
    "        return []\n",
    "    \n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    image_files = [f for f in os.listdir(dir_path) if any(f.lower().endswith(ext) for ext in valid_extensions)]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No valid image files found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images. Processing all...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, filename in enumerate(image_files, 1):\n",
    "        image_path = os.path.join(dir_path, filename)\n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "        \n",
    "        result = process_single_image(image_path)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"✓ Generated {len(result['tags'])} tags\")\n",
    "            print(f\"✓ Tags: {result['tags']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8340973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive image analysis...\n",
      "Found 5 images. Processing all...\n",
      "\n",
      "[1/5] Processing: image1.jpeg\n",
      "Processing: image1.jpeg\n",
      "✓ Generated 12 tags\n",
      "✓ Tags: ['india', 'army', 'image', 'bengal', 'png', 'article', 'visible', 'titled', 'news', 'flag', 'assam', 'indian']\n",
      "\n",
      "[2/5] Processing: image2.jpg\n",
      "Processing: image2.jpg\n",
      "✓ Generated 8 tags\n",
      "✓ Tags: ['says', 'clearance', 'store', 'sign', 'image', 'visible', 'window', 'sale']\n",
      "\n",
      "[3/5] Processing: image3.webp\n",
      "Processing: image3.webp\n",
      "✓ Generated 3 tags\n",
      "✓ Tags: ['sign', 'visible', 'image']\n",
      "\n",
      "[4/5] Processing: image4.jpg\n",
      "Processing: image4.jpg\n",
      "✓ Generated 5 tags\n",
      "✓ Tags: ['baby', 'image', 'elephant', 'her', 'visible']\n",
      "\n",
      "[5/5] Processing: image5.webp\n",
      "Processing: image5.webp\n",
      "✓ Generated 12 tags\n",
      "✓ Tags: ['past', 'mumbai', 'india', 'delhi', 'image', 'store', 'popular', 'place', 'people', 'bengal', 'visible', 'exterior']\n",
      "\n",
      "Creating single comprehensive report...\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "Total images processed: 5\n",
      "Comprehensive report saved: output_reports\\comprehensive_image_analysis_report.pdf\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "\n",
      "Image 1: image1.jpeg\n",
      "Tags (12): india, army, image, bengal, png, article, visible, titled, news, flag, assam, indian\n",
      "Caption: the flag of the indian army\n",
      "Text: Text extraction failed\n",
      "\n",
      "Image 2: image2.jpg\n",
      "Tags (8): says, clearance, store, sign, image, visible, window, sale\n",
      "Caption: a red up to 75 % off sign\n",
      "Text: Text extraction failed\n",
      "\n",
      "Image 3: image3.webp\n",
      "Tags (3): sign, visible, image\n",
      "Caption: a stop sign and a stop sign on a pole\n",
      "Text: Text extraction failed\n",
      "\n",
      "Image 4: image4.jpg\n",
      "Tags (5): baby, image, elephant, her, visible\n",
      "Caption: an elephant and her baby walking in a field\n",
      "Text: Text extraction failed\n",
      "\n",
      "Image 5: image5.webp\n",
      "Tags (12): past, mumbai, india, delhi, image, store, popular, place, people, bengal, visible, exterior\n",
      "Caption: people are walking in front of a store\n",
      "Text: Text extraction failed\n"
     ]
    }
   ],
   "source": [
    "# Cell 13\n",
    "data_directory = \"C:\\\\Users\\\\arnav\\\\Desktop\\\\LLM_Tagger\\\\data\"\n",
    "output_dir = ensure_output_dir()\n",
    "\n",
    "print(\"Starting comprehensive image analysis...\")\n",
    "all_results = process_all_images_in_directory(data_directory)\n",
    "\n",
    "if all_results:\n",
    "    print(f\"\\nCreating single comprehensive report...\")\n",
    "    pdf_path = create_single_comprehensive_report(all_results, output_dir)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ANALYSIS COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total images processed: {len(all_results)}\")\n",
    "    print(f\"Comprehensive report saved: {pdf_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    for i, result in enumerate(all_results, 1):\n",
    "        print(f\"\\nImage {i}: {result['filename']}\")\n",
    "        print(f\"Tags ({len(result['tags'])}): {', '.join(result['tags'])}\")\n",
    "        print(f\"Caption: {result['caption']}\")\n",
    "        print(f\"Text: {result['extracted_text'][:50]}{'...' if len(result['extracted_text']) > 50 else ''}\")\n",
    "else:\n",
    "    print(\"No images were successfully processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
