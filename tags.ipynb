{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "359b8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: reportlab in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.4.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: datasets in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: easyocr in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from reportlab) (3.4.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torchvision>=0.5 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (0.22.0)\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (4.12.0.88)\n",
      "Requirement already satisfied: scipy in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (1.15.3)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (0.25.2)\n",
      "Requirement already satisfied: python-bidi in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (0.6.6)\n",
      "Requirement already satisfied: Shapely in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (2.1.1)\n",
      "Requirement already satisfied: pyclipper in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from easyocr) (1.11.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from scikit-image->easyocr) (2025.5.24)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\arnav\\desktop\\llm_tagger\\venv\\lib\\site-packages (from scikit-image->easyocr) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "%pip install transformers torch pillow reportlab opencv-python datasets accelerate bitsandbytes easyocr pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a04088c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, DonutProcessor, VisionEncoderDecoderModel, LayoutLMv3Processor, LayoutLMv3ForTokenClassification, Kosmos2ForConditionalGeneration, AutoProcessor\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import easyocr\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a05068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "EasyOCR loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    import easyocr\n",
    "    ocr_reader = easyocr.Reader(['en'])\n",
    "    print(\"EasyOCR loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"EasyOCR failed: {e}\")\n",
    "    try:\n",
    "        import pytesseract\n",
    "        print(\"Using Tesseract as fallback\")\n",
    "        ocr_reader = None\n",
    "    except:\n",
    "        print(\"No OCR available, will use fallback method\")\n",
    "        ocr_reader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be1c5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BLIP model...\n",
      "Loading LayoutLMv3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Kosmos-2 model...\n",
      "All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "print(\"Loading BLIP model...\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "print(\"Loading LayoutLMv3 model...\")\n",
    "layoutlm_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "layoutlm_model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\").to(device)\n",
    "\n",
    "print(\"Loading Kosmos-2 model...\")\n",
    "kosmos_processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "kosmos_model = Kosmos2ForConditionalGeneration.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(device)\n",
    "\n",
    "print(\"All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "483d3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "def handle_path(input_path):\n",
    "    input_path = input_path.strip().strip('\"').strip(\"'\")\n",
    "    if os.path.isfile(input_path):\n",
    "        return input_path\n",
    "    elif os.path.isabs(input_path):\n",
    "        return input_path\n",
    "    else:\n",
    "        return os.path.abspath(input_path)\n",
    "\n",
    "def ensure_output_dir():\n",
    "    output_dir = \"output_reports\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    return output_dir\n",
    "\n",
    "def validate_image_path(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Path does not exist: {image_path}\")\n",
    "    if os.path.isdir(image_path):\n",
    "        raise IsADirectoryError(f\"Path is a directory, not a file: {image_path}\")\n",
    "    if not os.path.isfile(image_path):\n",
    "        raise FileNotFoundError(f\"Path is not a valid file: {image_path}\")\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    file_ext = os.path.splitext(image_path.lower())[1]\n",
    "    if file_ext not in valid_extensions:\n",
    "        raise ValueError(f\"Invalid image format. Supported: {valid_extensions}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44dc22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def extract_social_media_tags(caption):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', caption.lower())\n",
    "    \n",
    "    exclude_words = {'the', 'and', 'with', 'for', 'are', 'that', 'this', 'has', 'was', 'from', 'they', 'have', 'been', 'will', 'can', 'said', 'each', 'which', 'more', 'also', 'its', 'would', 'may', 'about', 'out', 'many', 'time', 'very', 'when', 'much', 'new', 'some', 'could', 'state', 'other', 'after', 'first', 'well', 'way', 'even', 'years', 'work', 'through', 'over', 'government', 'into', 'than', 'because', 'most', 'only', 'over', 'think', 'also', 'back', 'after', 'use', 'two', 'how', 'our', 'work', 'life', 'way', 'even', 'back', 'any', 'good', 'woman', 'through', 'just', 'form', 'sentence', 'great', 'think', 'say', 'help', 'low', 'line', 'differ', 'turn', 'cause', 'much', 'mean', 'before', 'move', 'right', 'boy', 'old', 'too', 'same', 'tell', 'does', 'set', 'three', 'want', 'air', 'well', 'also', 'play', 'small', 'end', 'put', 'home', 'read', 'hand', 'port', 'large', 'spell', 'add', 'even', 'land', 'here', 'must', 'big', 'high', 'such', 'follow', 'act', 'why', 'ask', 'men', 'change', 'went', 'light', 'kind', 'off', 'need', 'house', 'picture', 'try', 'again', 'animal', 'point', 'mother', 'world', 'near', 'build', 'self', 'earth', 'father', 'head', 'stand', 'own', 'page', 'should', 'country', 'found', 'answer', 'school', 'grow', 'study', 'still', 'learn', 'plant', 'cover', 'food', 'sun', 'four', 'between', 'state', 'keep', 'eye', 'never', 'last', 'let', 'thought', 'city', 'tree', 'cross', 'farm', 'hard', 'start', 'might', 'story', 'saw', 'far', 'sea', 'draw', 'left', 'late', 'run', 'while', 'press', 'close', 'night', 'real', 'life', 'few', 'north', 'open', 'seem', 'together', 'next', 'white', 'children', 'begin', 'got', 'walk', 'example', 'ease', 'paper', 'group', 'always', 'music', 'those', 'both', 'mark', 'often', 'letter', 'until', 'mile', 'river', 'car', 'feet', 'care', 'second', 'book', 'carry', 'took', 'science', 'eat', 'room', 'friend', 'began', 'idea', 'fish', 'mountain', 'stop', 'once', 'base', 'hear', 'horse', 'cut', 'sure', 'watch', 'color', 'face', 'wood', 'main', 'enough', 'plain', 'girl', 'usual', 'young', 'ready', 'above', 'ever', 'red', 'list', 'though', 'feel', 'talk', 'bird', 'soon', 'body', 'dog', 'family', 'direct', 'leave', 'song', 'measure', 'door', 'product', 'black', 'short', 'numeral', 'class', 'wind', 'question', 'happen', 'complete', 'ship', 'area', 'half', 'rock', 'order', 'fire', 'south', 'problem', 'piece', 'told', 'knew', 'pass', 'since', 'top', 'whole', 'king', 'space', 'heard', 'best', 'hour', 'better', 'during', 'hundred', 'five', 'remember', 'step', 'early', 'hold', 'west', 'ground', 'interest', 'reach', 'fast', 'verb', 'sing', 'listen', 'six', 'table', 'travel', 'less', 'morning', 'ten', 'simple', 'several', 'vowel', 'toward', 'war', 'lay', 'against', 'pattern', 'slow', 'center', 'love', 'person', 'money', 'serve', 'appear', 'road', 'map', 'rain', 'rule', 'govern', 'pull', 'cold', 'notice', 'voice', 'unit', 'power', 'town', 'fine', 'certain', 'fly', 'fall', 'lead', 'cry', 'dark', 'machine', 'note', 'wait', 'plan', 'figure', 'star', 'box', 'noun', 'field', 'rest', 'correct', 'able', 'pound', 'done', 'beauty', 'drive', 'stood', 'contain', 'front', 'teach', 'week', 'final', 'gave', 'green', 'quick', 'develop', 'ocean', 'warm', 'free', 'minute', 'strong', 'special', 'mind', 'behind', 'clear', 'tail', 'produce', 'fact', 'street', 'inch', 'multiply', 'nothing', 'course', 'stay', 'wheel', 'full', 'force', 'blue', 'object', 'decide', 'surface', 'deep', 'moon', 'island', 'foot', 'system', 'busy', 'test', 'record', 'boat', 'common', 'gold', 'possible', 'plane', 'stead', 'dry', 'wonder', 'laugh', 'thousands', 'ago', 'ran', 'check', 'game', 'shape', 'equate', 'hot', 'miss', 'brought', 'heat', 'snow', 'tire', 'bring', 'yes', 'distant', 'fill', 'east', 'paint', 'language', 'among'}\n",
    "    \n",
    "    filtered_tags = [word for word in words if word not in exclude_words and len(word) > 2]\n",
    "    \n",
    "    return list(set(filtered_tags))[:15]\n",
    "\n",
    "def generate_tags_with_blip(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = blip_model.generate(**inputs, max_length=50, num_beams=4)\n",
    "    \n",
    "    caption = blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    tags = extract_social_media_tags(caption)\n",
    "    \n",
    "    return tags, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f014229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "def extract_text_with_ocr(image_path):\n",
    "    try:\n",
    "        if ocr_reader is not None:\n",
    "            results = ocr_reader.readtext(image_path)\n",
    "            extracted_texts = []\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence > 0.3:\n",
    "                    cleaned_text = text.strip()\n",
    "                    if len(cleaned_text) > 1:\n",
    "                        extracted_texts.append(cleaned_text)\n",
    "            final_text = ' '.join(extracted_texts)\n",
    "            return final_text if final_text else \"No text detected\"\n",
    "        else:\n",
    "            try:\n",
    "                import pytesseract\n",
    "                from PIL import Image\n",
    "                image = Image.open(image_path)\n",
    "                text = pytesseract.image_to_string(image)\n",
    "                return text.strip() if text.strip() else \"No text detected\"\n",
    "            except:\n",
    "                return \"Text extraction unavailable\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"Text extraction failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f31a23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "def verify_tags_with_layoutlm(image_path, tags, extracted_text):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        all_words = tags + extracted_text.split()\n",
    "        words = [word.lower() for word in all_words if word.isalpha() and len(word) > 2][:100]\n",
    "        \n",
    "        if not words:\n",
    "            return tags\n",
    "        \n",
    "        inputs = layoutlm_processor(image, words, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = layoutlm_model(**inputs)\n",
    "        \n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        verified_tags = []\n",
    "        \n",
    "        for i, word in enumerate(words[:min(len(words), predictions.shape[1])]):\n",
    "            if predictions[0][i].max().item() > 0.25:\n",
    "                verified_tags.append(word)\n",
    "        \n",
    "        unique_verified = list(set(verified_tags))\n",
    "        return unique_verified[:12] if unique_verified else tags[:12]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return tags[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f6cb700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def generate_clean_explanation_with_kosmos(image_path, verified_tags):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        prompt = \"What do you see in this image?\"\n",
    "        \n",
    "        inputs = kosmos_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = kosmos_model.generate(\n",
    "                **inputs,\n",
    "                max_length=80,\n",
    "                do_sample=False,\n",
    "                num_beams=1\n",
    "            )\n",
    "        \n",
    "        full_text = kosmos_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        clean_text = full_text.replace(prompt, \"\").strip()\n",
    "        \n",
    "        if len(clean_text) < 15 or any(word in clean_text.lower() for word in ['grounding', 'bbox', 'coord']):\n",
    "            clean_text = f\"This image shows {', '.join(verified_tags[:4])}. The visual elements include various objects and features that define the scene's composition and context.\"\n",
    "        \n",
    "        sentences = [s.strip() + '.' for s in clean_text.split('.') if len(s.strip()) > 5]\n",
    "        final_text = ' '.join(sentences[:2]) if sentences else clean_text\n",
    "        \n",
    "        return final_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"This image contains {', '.join(verified_tags[:4])}. Visual analysis identifies key elements and features present in the scene.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2022728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "def create_pdf_report(image_path, tags, extracted_text, verified_tags, explanation, output_dir):\n",
    "    filename = os.path.basename(image_path).split('.')[0]\n",
    "    pdf_path = os.path.join(output_dir, f\"{filename}_analysis.pdf\")\n",
    "    \n",
    "    doc = SimpleDocTemplate(pdf_path, pagesize=letter, topMargin=36, bottomMargin=36)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    times_style = ParagraphStyle(\n",
    "        'TimesRoman',\n",
    "        parent=styles['Normal'],\n",
    "        fontName='Times-Roman',\n",
    "        fontSize=11,\n",
    "        spaceAfter=4,\n",
    "        spaceBefore=0\n",
    "    )\n",
    "    \n",
    "    title_style = ParagraphStyle(\n",
    "        'TimesTitle',\n",
    "        parent=styles['Title'],\n",
    "        fontName='Times-Bold',\n",
    "        fontSize=14,\n",
    "        spaceAfter=8,\n",
    "        spaceBefore=0\n",
    "    )\n",
    "    \n",
    "    story = []\n",
    "    story.append(Paragraph(\"Intelligent Image Analysis Report\", title_style))\n",
    "    story.append(Spacer(1, 8))\n",
    "    \n",
    "    try:\n",
    "        img = RLImage(image_path, width=280, height=200)\n",
    "        story.append(img)\n",
    "    except:\n",
    "        story.append(Paragraph(\"Image embedding failed\", times_style))\n",
    "    \n",
    "    story.append(Spacer(1, 8))\n",
    "    story.append(Paragraph(f\"<b>AI-Generated Tags:</b> {', '.join(tags)}\", times_style))\n",
    "    story.append(Spacer(1, 4))\n",
    "    story.append(Paragraph(f\"<b>Extracted Text:</b> {extracted_text}\", times_style))\n",
    "    story.append(Spacer(1, 4))\n",
    "    story.append(Paragraph(f\"<b>Verified Tags:</b> {', '.join(verified_tags)}\", times_style))\n",
    "    story.append(Spacer(1, 4))\n",
    "    story.append(Paragraph(f\"<b>Explanation:</b> {explanation}\", times_style))\n",
    "    \n",
    "    doc.build(story)\n",
    "    return pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "faa4f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "def process_single_image(image_path):\n",
    "    try:\n",
    "        image_path = handle_path(image_path)\n",
    "        validate_image_path(image_path)\n",
    "        output_dir = ensure_output_dir()\n",
    "        \n",
    "        print(f\"Processing: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        tags, caption = generate_tags_with_blip(image_path)\n",
    "        extracted_text = extract_text_with_ocr(image_path)\n",
    "        verified_tags = verify_tags_with_layoutlm(image_path, tags, extracted_text)\n",
    "        explanation = generate_clean_explanation_with_kosmos(image_path, verified_tags)\n",
    "        \n",
    "        pdf_path = create_pdf_report(image_path, tags, extracted_text, verified_tags, explanation, output_dir)\n",
    "        \n",
    "        return {\n",
    "            'filename': os.path.basename(image_path),\n",
    "            'tags': tags,\n",
    "            'caption': caption,\n",
    "            'extracted_text': extracted_text,\n",
    "            'verified_tags': verified_tags,\n",
    "            'explanation': explanation,\n",
    "            'pdf_report': pdf_path\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94c9e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "def process_all_images_in_directory(dir_path):\n",
    "    dir_path = handle_path(dir_path)\n",
    "    \n",
    "    if not os.path.isdir(dir_path):\n",
    "        print(\"Invalid directory path\")\n",
    "        return []\n",
    "    \n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    image_files = [f for f in os.listdir(dir_path) if any(f.lower().endswith(ext) for ext in valid_extensions)]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"No valid image files found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images. Processing all...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, filename in enumerate(image_files, 1):\n",
    "        image_path = os.path.join(dir_path, filename)\n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "        \n",
    "        result = process_single_image(image_path)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            print(f\"✓ Tags: {result['tags'][:6]}\")\n",
    "            print(f\"✓ Text: {result['extracted_text'][:60]}...\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "355d3c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 images. Processing all...\n",
      "\n",
      "[1/5] Processing: image1.jpeg\n",
      "Processing: image1.jpeg\n",
      "✓ Tags: ['indian', 'army', 'flag']\n",
      "✓ Text: No text detected...\n",
      "\n",
      "[2/5] Processing: image2.jpg\n",
      "Processing: image2.jpg\n",
      "✓ Tags: ['store', 'sign', 'clothing']\n",
      "✓ Text: Up to 52499 up 5T{ '2499 the markot Mem CLEARANCE...\n",
      "\n",
      "[3/5] Processing: image3.webp\n",
      "Processing: image3.webp\n",
      "✓ Tags: ['pole', 'sign']\n",
      "✓ Text: Grey Fox Tr Waterfall Dr STOP...\n",
      "\n",
      "[4/5] Processing: image4.jpg\n",
      "Processing: image4.jpg\n",
      "✓ Tags: ['baby', 'elephant', 'walking', 'her']\n",
      "✓ Text: No text detected...\n",
      "\n",
      "[5/5] Processing: image5.webp\n",
      "Processing: image5.webp\n",
      "✓ Tags: ['store', 'people', 'walking']\n",
      "✓ Text: pepperfry com IG Store...\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE - 5 images processed\n",
      "============================================================\n",
      "\n",
      "Image 1: image1.jpeg\n",
      "Tags: ['indian', 'army', 'flag']\n",
      "Text Found: No text detected\n",
      "Verified Tags: ['indian', 'army', 'flag']\n",
      "PDF: image1_analysis.pdf\n",
      "\n",
      "Image 2: image2.jpg\n",
      "Tags: ['store', 'sign', 'clothing']\n",
      "Text Found: Up to 52499 up 5T{ '2499 the markot Mem CLEARANCE\n",
      "Verified Tags: ['store', 'sign', 'clothing']\n",
      "PDF: image2_analysis.pdf\n",
      "\n",
      "Image 3: image3.webp\n",
      "Tags: ['pole', 'sign']\n",
      "Text Found: Grey Fox Tr Waterfall Dr STOP\n",
      "Verified Tags: ['pole', 'sign']\n",
      "PDF: image3_analysis.pdf\n",
      "\n",
      "Image 4: image4.jpg\n",
      "Tags: ['baby', 'elephant', 'walking', 'her']\n",
      "Text Found: No text detected\n",
      "Verified Tags: ['baby', 'elephant', 'walking', 'her']\n",
      "PDF: image4_analysis.pdf\n",
      "\n",
      "Image 5: image5.webp\n",
      "Tags: ['store', 'people', 'walking']\n",
      "Text Found: pepperfry com IG Store\n",
      "Verified Tags: ['store', 'people', 'walking']\n",
      "PDF: image5_analysis.pdf\n"
     ]
    }
   ],
   "source": [
    "# Cell 13\n",
    "data_directory = \"C:\\\\Users\\\\arnav\\\\Desktop\\\\LLM_Tagger\\\\data\"\n",
    "all_results = process_all_images_in_directory(data_directory)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PROCESSING COMPLETE - {len(all_results)} images processed\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"\\nImage {i}: {result['filename']}\")\n",
    "    print(f\"Tags: {result['tags']}\")\n",
    "    print(f\"Text Found: {result['extracted_text']}\")\n",
    "    print(f\"Verified Tags: {result['verified_tags']}\")\n",
    "    print(f\"PDF: {os.path.basename(result['pdf_report'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a46f998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fallback OCR method\n"
     ]
    }
   ],
   "source": [
    "# Cell 14\n",
    "def simple_text_extraction_fallback(image_path):\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import pytesseract\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image, lang='eng+hin')\n",
    "        return text.strip() if text.strip() else \"No text detected\"\n",
    "    except:\n",
    "        return \"No text extraction available\"\n",
    "\n",
    "def extract_text_with_ocr(image_path):\n",
    "    return simple_text_extraction_fallback(image_path)\n",
    "\n",
    "print(\"Using fallback OCR method\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
